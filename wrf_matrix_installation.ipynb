{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1BaxwhJ3eJ-rWIr9F4fKiPYMpsx-b-ayZ","timestamp":1665770354031}],"authorship_tag":"ABX9TyPOy23ohOl/TdwSeinbD1By"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This document provides instructions for the installation and running of WRF version 4.3\n","\n","If you have questions, feel free to contact Christopher Phillips, chris.phillips@uah.edu\n","\n","Another useful source is the WRF user manual: https://www2.mmm.ucar.edu/wrf/users/docs/user_guide_v4/contents.html\n","\n","Note that if you are running WRF on the UAH Matrix system you need to use the slurm system to issue the geogrid.exe, ungrib.exe, metgrid.exe, real.exe and wrf.exe commands. Exapmle slurm scripts are provided at the end of this guide.\n","\n","--------------------\n","# --- Installing WRF ---\n","\n","\n","Prior to installing WRF, you will need the following modules loaded:\n","\n","    module load null\n","    module load netcdf/4.4.0_mpi-pgi16.3_gcc\n","    module load ncarg/6.3_mpi-pgi16.3_gcc\n","    module load hdf5/v1.8.17_mpi-pgi16.3_gcc\n","    module load pgi/16.3\n","    module load mpich/3.2-pgi16.3\n","\n","Additionally, you will need the following variables set:\n","\n","    setenv WRF_EM_CORE 1\n","    setenv WRF_NMM_CORE 0\n","    setenv WRFIO_NCD_LARGE_FILE_SUPPORT 1\n","    setenv ENVCOMPDEFS -DWRF_CHEM\n","    setenv WRF_CHEM 0\n","    setenv WRF_KPP 0\n","\n","    setenv JASPERLIB /common/pkgs/jasper/1.900.1-pgi12.10-EL6_64-201212201430/lib\n","    setenv JASPERINC /common/pkgs/jasper/1.900.1-pgi12.10-EL6_64-201212201430/include\n","\n","Note that if you want to use chemistry, set WRF_CHEM to 1 instead of 0. It is recommended to place the module and setenv statements into the .tcshrc.matrix file located in your home directory.\n","\n","While logged into Matrix, create a new directory for the wrf system and download the latest version\n","    \n","    mkdir wrf_model\n","    cd wrf_model\n","    git clone https://github.com/wrf-model/WRF\n","\n","Then cd into the new WRF directory and run the configuration script\n","\n","    ./configure\n","\n","Select option 3 for parallel processing using the PGI compilers. When prompted, select no nesting. At this point, the following message should appear\n","\n","    Testing for NetCDF, C and Fortran compiler\n","\n","    This installation of NetCDF is 64-bit\n","        C compiler is 64-bit\n","        Fortran compiler is 64-bit\n","        It will build in 64-bit\n","\n","Now open the newly created configure.wrf file\n","\n","    vi configure.wrf\n","\n","And make the following changes\n","\n","1) SFC = mpifort\n","\n","2) SCC = h5pcc\n","\n","3) h5pcc\n","\n","4) DM_FC = mpifort\n","\n","5) DM_CC = h5pcc -DMPI2_SUPPORT\n","\n","6) Add -Kieee to the FCOPTIM variable. Do NOT delete any other flags.\n","\n","    FCOPTIM = -O3 -Kieee #-fastsse -Mvect=noaltcode -Msmartalloc -Mprefetch=distance:8 -Mfprelaxed # -Minfo=all =Mneginfo=all\n","\n","Next, run the compilation script and wait. This can take anywhere from 30 minutes to 1.5 hours.\n","\n","    ./compile em_real >& compile_real.log\n","\n","After compilation, check the log file for successful build of the WRF executables.\n","\n","    tail compile_scm.log\n","\n","You should see something similar to the below:\n","\n","    --->                  Executables successfully built                  <---\n","\n","    -rwxr-x--- 1 cphillip cphillip 131990352 Oct 19 15:13 main/real.exe\n","    -rwxr-x--- 1 cphillip cphillip 159422160 Oct 19 15:13 main/wrf.exe\n","\n","If so, congratulations, you have compile the WRF. If not, then check the log file for errors (searching for \"severe\" is the easiest way) and correct them.\n","\n","Next, we will install the WRF Pre-processing system (WPS)."],"metadata":{"id":"WtAL_R0TaLWR"}},{"cell_type":"markdown","source":["-----------------------------\n","#--- Installing the WPS ---\n","\n","The WPS is the system used to prepare analysis data from other models such as the GFS or HRRR for input into the WRF model. The details for running the WPS are in a later section.\n","\n","Change directories again to the wrf43 directory\n","\n","\n","```\n","cd ../\n","```\n","\n","And clone the WPS directory.\n","\n","```\n","git clone https://github.com/wrf-model/WPS\n","cd WPS\n","```\n","\n","Similar to the WRF model, run the configuration script:\n","\n","```\n","./configure\n","```\n","\n","and select option 7 when prompted. You should see similar output:\n","\n","```\n","------------------------------------------------------------------------\n","Configuration successful. To build the WPS, type: compile\n","------------------------------------------------------------------------\n","\n","Testing for NetCDF, C and Fortran compiler\n","\n","This installation NetCDF is 64-bit\n","C compiler is 64-bit\n","Fortran compiler is 64-bit\n","```\n","\n","When configuration is complete make the following changes to configure.wps:\n","\n","1) SFC             =       mpifort\n","\n","2) SCC             =       h5pcc\n","\n","3) DM_FC           =       mpifort\n","\n","4) DM_CC           =       h5pcc -DMPI2_SUPPORT\n","\n","\n","\n","Then compile.\n","\n","```\n","./compile >& compile_wps.log\n","```\n","\n","Unlike WRF, there is no clear completion message at the end of the log file, but ```ls -l``` should show links to 3 exectuables:\n","\n","```\n","geogrid.exe -> geogrid/src/geogrid.exe\n","metgrid.exe -> metgrid/src/metgrid.exe\n","ungrib.exe -> ungrib/src/ungrib.exe\n","```\n","\n","If these are not created, check the log files for error and re-compile. You can run ```./clean``` to clear out the intermediate compilation files between compile attempts.\n","\n","Congratulations, you have now compiled both the WRF model and the WPS system."],"metadata":{"id":"ZAmpTVDiqJaT"}},{"cell_type":"markdown","source":["-------------------------------------------------------\n","# --- Downloading Static Fields ---\n","The next step is to download and unpack the so-called- static input fields. These largely consist of terrian and topography data sets. We'll place them in the model root directory, wrf_model.\n","\n","```\n","cd ..\n","mkdir static_data\n","wget https://www2.mmm.ucar.edu/wrf/src/wps_files/geog_high_res_mandatory.tar.gz\n","tar -xvzf geog_high_res_mandatory.tar.gz\n","```\n","\n","This will likely take a while as there are many files in this data set. Once it is complete. You're ready to prepare your first simulation."],"metadata":{"id":"P8_gbYHOJuq1"}},{"cell_type":"markdown","source":["--------------------\n","#--- Setting Up Your First WRF Simulation ---\n","\n","Running a simulation is reasonably straight-forward. The steps are:\n","\n","1. Determine your domain\n","2. Download suitable data for boundary conditions.\n","3. Create the grid for your model and grid the input data using WPS.\n","4. Run real.exe to convert the WPS files into WRF input files.\n","5. Run wrf.exe.\n","6. Bask in the radiance of your completed WRF simulation\n","\n","Each of these steps though has sub-parts and questions that you must give thought to, and the answers to those questions vary depending on the goals of your simulation. Weather modeling is both art and science.\n","\n","# Determine your domain\n","\n","We'll start with an exciting case. Let's simulate the December 13th, 2022 tornado outbreak over Lousiana and Mississippi. Some questions to consider:\n","\n","- How large should the domain be?\n","- What grid spacing should we use?\n","- How many nests should there be?\n","\n","As a rule of thumb, the domain should be large enough that features on the edges don't propagate into the center. Model edges can be weird becase data is interpolated from a larger model to the WRF grid. To compute this, we use:\n","\n","$ Domain~Width~(km) = \\frac{10~(m/s)~*~Simulation~Time~(s)}{1000 ~(m/km)} * 2$\n","\n","Those who have taken a mid-latitude synoptic meteorology class might recognize 10 m/s as the scale speed of the zonal wind. Meridinal flow is typically about 5 m/s, so you can make the model height about half the width.\n","\n","*NOTE: These are first-guesses only. Grid parameters can be optimized well beyond these rules.*\n","\n","For a tornado outbreak, convection will be of prime-imprtance, so we need a grid spacing capable of resolving convection. We'll set grid spacing to 1 km. To get down from GFS's low resolution of 0.25 deg. We'll use three nests. The outer domain will be 9 km spacing, the second 3 km, and the innermost 1 km. Note the ratio between grids. WRF likes ratios of 3 and 5. These values provide a better interpolation at the nest edges. An outermost grid spacing of 9km is also about 3 times greater than GFS's native resolution, preserving this pattern even at the outermost edge.\n","\n","# Download GFS analysis for boundary conditions\n","The historical GFS archive can be accessed through: https://rda.ucar.edu/datasets/ds084-1/\n","\n","You will need an account, but they are free.\n","\n","Located and download the following files. I suggest placing them inside an \"input\" directory within your WPS directory for now.\n","- gfs.0p25.2022121300.f000.grib2\n","- gfs.0p25.2022121300.f003.grib2\n","- gfs.0p25.2022121300.f006.grib2\n","- gfs.0p25.2022121300.f009.grib2\n","- gfs.0p25.2022121300.f012.grib2\n","- gfs.0p25.2022121300.f015.grib2\n","- gfs.0p25.2022121300.f018.grib2\n","- gfs.0p25.2022121300.f021.grib2\n","- gfs.0p25.2022121300.f024.grib2\n","\n","These are 24 hours worth of GFS forecast output that will be used to supply the boundary conditoins for the WRF simulation. The next step is to run the WPS."],"metadata":{"id":"Qb0oJwunSxJh"}},{"cell_type":"markdown","source":["-----------------------\n","# --- Running WPS ---\n","\n","Running the WPS consists of the following steps:\n","\n","1. Create the namelist.wps file\n","2. Link the proper VTable file\n","3. Link the input data\n","4. Run geogrid\n","5. Run ungrib\n","6. Run metgrid\n","\n","It can be noted that geogrid is its own program that only depends on the namelist.wps file, so it can be run anytime after creating the namelist but before running metgrid.\n","\n","Create a document in the WPS directory and name it \"namelist.wps\"\n","\n","Enter the following information except for anything after a \"#.\" Those are my comments. In the future, the WRFDomainWizard is an excellent tool for creating your grids. https://esrl.noaa.gov/gsd/wrfportal/DomainWizardForLAPS.html\n","\n","```\n","&share\n"," wrf_core = 'ARW',            # Which dynamical core to use. ARW is suitable for nearly every task.\n"," max_dom = 3,                 # The number of domains\n"," start_date = '2022-12-13_00:00:00', '2022-12-13_00:00:00', '2022-12-13_00:00:00' # Start and end dates, inclusive.\n"," end_date = '2022-12-14_00:00:00', '2022-12-14_00:00:00', '2022-12-14_00:00:00'   # One for each domain.\n"," interval_seconds = 10800     # How often to read a boundary file. GFS outputs every 3 hours (10800 seconds).\n","/\n","\n","&geogrid\n"," parent_id         =   1,   1,   2,   # Which grid the nest belongs to.\n"," parent_grid_ratio =   1,   3,   3,   # The ratio to use for nest's grid spacing\n"," i_parent_start    =   1,  60, 83        # The grid coordinate that the nest starts at\n"," j_parent_start    =   1,  50, 84,\n"," e_we              =  200, 241, 226,       # How many points are in the grid.\n"," e_sn              =  180, 244, 229,       # Nest dimensions should be one more than a multiple of the ratio.\n"," geog_data_res = 'default','default', # The resolution of the static data to read. Default is fine.\n"," dx = 9000,                          # Grid spacing of the outermost grid\n"," dy = 9000,\n"," map_proj = 'lambert',                # Map projection. Lambert for mid-lats, mercator for tropics, polar for poles.\n"," ref_lat   =  31.784,                  # Grid center\n"," ref_lon   = -91.387,\n"," truelat1  =  31.784,                   # Lats/lons where a glob and the projection are \"1 to 1\" so to speak.\n"," truelat2  =  31.784,\n"," stand_lon = -91.387,                   # Keep the same as your center longitude.\n"," geog_data_path = '/path_to_static_fields/WPS_GEOG/'\n","/\n","\n","&ungrib\n"," out_format = 'WPS'\n"," prefix = 'FILE'\n","/\n","\n","&metgrid\n"," fg_name = 'FILE'\n","/\n","\n","```\n","\n","Once the namelist.wps file is created, link the vtable using the following command:\n","```\n","ln -sf ungrib/Variable_Tables/Vtable.GFS ./Vtable\n","```\n","\n","You can check the link by running ls -l inside your WPS directory.\n","```\n","ls -l\n","Vtable -> ungrib/Variable_Tables/Vtable.GFS\n","```\n","\n","Now inside your WPS directory, link the GFS input by running the following command:\n","```\n","./link_grib.csh input/gfs.*\n","```\n","A quick ls should show the GFS files linked into the WPS directory.\n","Next run geogrid, ungrib, and metgrid.\n","```\n","./geogrid.exe > geogrid.log\n","./ungrib.exe > ungrib.log\n","./metgrid > metgrid.log\n","```\n","\n","At each step, ls should reveal new files.\n","Geogrid creates \"geo_em\" files. Ungrib creates \"FILE\" files. Metgrid creates \"met_em\" files.\n","\n","\"met_em\" files are what you feed into the WRF real.exe and are the ultimate goal of running the WPS system."],"metadata":{"id":"b2Yp1yZeOnG6"}},{"cell_type":"markdown","source":["-------------------------\n","#--- Running WRF ---\n","\n","With the WPS finished, running WRF is fairly straight-forward, and consists of only a few steps.\n","\n","1. Create the WRF namelist\n","2. Link the met_em files\n","3. Run real.exe to create wrf_input files\n","4. Run wrf.exe to create that sweet, sweet output.\n","\n","Creating the namelist is a simple task because it is based on the WPS namelist. cd into the WRF/run directory.. The run directory is where you will run WRF and create the various namelist and input files. Open the namelist.input file. The namelist is a long file, so I'm only going to list the variables that you should check/change from the template. For more information on the available variables and what they do, see chapter 5 of the WRF user guide. Again, don't add comments behind a \"#\" symbol.\n","\n","```\n","&time_control\n","  run_days = 0,\n","  run_hours = 24,\n","  run_minutes = 0,\n","  run_seconds = 0,\n","  start_year = 2022, 2022, 2022,\n","  start_month = 12, 12, 12,\n","  start_day = 13, 13, 13,\n","  start_hour = 0, 0, 0,\n","  start_minute = 0, 0, 0,\n","  start_second = 0, 0, 0,\n","  end_year = 2022, 2022, 2022,\n","  end_month = 12, 12, 12,\n","  end_day = 14, 14, 14,\n","  end_hour = 0, 0, 0,\n","  end_minute = 0, 0, 0,\n","  end_second = 0, 0, 0,\n","  interval_seconds = 10800\n","  history_interval = 180, 60, 15, # This is the output interval in minutes for each grid.\n","  frames_per_outfile = 1, 1, 1,   # This is how many timesteps to put in one file. Do one. It's easier all around.\n","/\n","\n","&domains\n","  time_step = 54, # Model dt in seconds. Should divide evenly into history_interval.\n","  max_dom = 3,    # Number of grids. Often set to 1 (outer domain only) for testing then reset for the final run.\n","  e_we = 200, 241, 226, # Note these match namelist.wps\n","  e_sn = 180, 244, 229,\n","  e_vert = 45, 45, 45,  # The number of vertical levels in the model. Not in namelist.wps\n","  num_metgrid_levels = 34, # How many levels are in the met_em files. Changes depending on source data.\n","  dx = 9000, 3000, 1000,\n","  dy = 9000, 3000, 1000,\n","  grid_id = 1, 2, 3,\n","  parent_id = 1, 1, 2,\n","  i_parent_start = 1, 60, 83,    # What grid point to start the nest\n","  j_parent_start = 1, 50, 84,\n","  parent_grid_ratio = 1, 3, 3,\n","  parent_time_Step_ratio = 1, 3, 3, # Keep in sync with the grid_ratio\n","  feedback = 1, # If 1, nested grids will provide input to the outergrid. This is usually good.\n","/\n","\n","&physics\n","  mp_physics =  8, 8, 8\n","  cu_physics = 1, 0, 0,\n","  ra_lw_physics = 4, 4, 4,\n","  ra_sw_physics = 4, 4, 4,\n","  bl_pbl_physics = 2, 2, 2,\n","  do_radar_ref = 1, # This tells the model to output simulated refl. It's slow, but nice for convective studies.\n","/\n","\n","```\n","\n","There are many, many more options. Many aren't even in the template. If you plan to use WRF regularly, give chapter 5 of the user manual a read through. There's a lot of valuable information there. Namelist settings are a blend of art and science. Play around to see what happens.\n","\n","The next step is to link the met_em files.\n","```\n","ln -sf ../WPS/met_em* ./\n","```\n","\n","Then run real.exe\n","```\n","real.exe\n","```\n","\n","This should create files similar to the following:\n","```\n","ls -l wrfinput_*\n","wrfinput_d01\n","wrfinput_d02\n","wrfinput_d03\n","```\n","\n","With the input files created, run wrf.exe. This will create output files and rsl log files.\n","\n","That's it. That's how you run WRF. Welcome to modeling world where the data is imaginary but we use it anyway.\n"],"metadata":{"id":"KOTwyRZDOtZY"}},{"cell_type":"markdown","source":["# SAMPLE SLURM SCRIPTS FOR MATRIX"],"metadata":{"id":"InP81NS4bQhx"}},{"cell_type":"markdown","source":["For WPS\n","\n","```\n"," #!/bin/bash\n"," ###Slurm Batch Script\n","\n"," ###Email Address\n"," #SBATCH --mail-user=your_email@uah.edu\n","\n"," ###Job Name\n"," #SBATCH -J mkgrid\n","\n"," ### Partition (queue), select shared or standard\n"," #SBATCH -p shared\n","\n"," ###TOTAL processors\n"," #SBATCH --ntasks 1\n","\n"," ###Total run time estimate (D-HH:MM)\n"," #SBATCH -t 2-00:00\n","\n","\n"," ### memory (GB per CPU)\n"," #SBATCH --mem-per-cpu=12G\n","\n","\n"," ### Mail to user on an event\n"," ### common options are FAIL, BEGIN, END, REQUEUE\n"," #SBATCH --mail-type=BEGIN,END,FAIL\n","\n"," ### Output files\n"," #SBATCH -o slurm-%j.out #STDOUT\n"," #SBATCH -e slurm-%j.err #STDERR\n","\n"," cd your_wps_dir\n"," ./geogrid.exe > geogrid.log\n"," ./link_grib.csh input/gfs.*\n"," ./ungrib.exe > ungrib.log\n"," ./metgrid.exe > metgrid.log\n","\n","```\n","\n","For WRF\n","\n","Run real.exe and wrf.exe separately since they both use the rsl log files.\n","\n","```\n"," #!/bin/bash\n"," ###Slurm Batch Script\n","\n"," ###Email Address\n"," #SBATCH --mail-user=your_email@uah.edu\n","\n"," ###Job Name\n"," #SBATCH -J WRFsim\n","\n"," ### Partition (queue), select shared or standard\n"," #SBATCH -p standard\n","\n"," ###TOTAL processors\n"," #SBATCH --ntasks 48\n","\n"," ###Total run time estimate (D-HH:MM)\n"," #SBATCH -t 7-00:00\n","\n"," ### memory (GB per CPU)\n"," #SBATCH --mem-per-cpu=8G\n","\n"," ### Mail to user on an event\n"," ### common options are FAIL, BEGIN, END, REQUEUE\n"," #SBATCH --mail-type=BEGIN,END,FAIL\n","\n"," ### Output files\n"," #SBATCH -o slurm-%j.out #STDOUT\n"," #SBATCH -e slurm-%j.err #STDERR\n","\n"," cd your_wrf_dir/run\n"," #srun ./real.exe\n"," srun ./wrf.exe\n","```"],"metadata":{"id":"X0H83P4Kbpzw"}},{"cell_type":"code","source":[],"metadata":{"id":"dCcpnyI_cDDn"},"execution_count":null,"outputs":[]}]}